{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "470e1ea9",
   "metadata": {},
   "source": [
    "미니배치, 이터레이션, 에포크 참조  \n",
    "https://wikidocs.net/55580"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef8e6284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import TensorDataset # 텐서데이터셋\n",
    "from torch.utils.data import DataLoader # 데이터로더\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b57c0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x_data = [[73, 80, 75],\n",
    "                       [93, 88, 93],\n",
    "                       [89, 91, 90],\n",
    "                       [96, 98, 100],\n",
    "                       [73, 66, 70]]\n",
    "        self.y_data = [[152], [185], [180], [196], [142]]\n",
    "\n",
    "      # 총 데이터의 개수를 리턴\n",
    "    def __len__(self): \n",
    "        return len(self.x_data)\n",
    "\n",
    "      # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n",
    "    def __getitem__(self, idx): \n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b8671b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiabetesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 커스텀 데이터셋 클래스의 생성자\n",
    "        # 데이터를 불러오고 전처리 한다.\n",
    "        xy = np.loadtxt('./data/diabetes.csv.gz', delimiter=',', dtype=np.float32)\n",
    "        self.x_data = torch.from_numpy(xy[:, 0:-1])\n",
    "        self.y_data = torch.from_numpy(xy[:, [-1]])\n",
    "        print(f'X\\'s shape: {self.x_data.shape} | Y\\'s shape: {self.y_data.shape}')\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 길이를 반환하는 메소드\n",
    "        return len(self.x_data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # 데이터셋에서 하나의 특정 샘플 (x, y)을 가져오는 메소드\n",
    "        return self.x_data[idx], self.y_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d299e115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X's shape: torch.Size([759, 8]) | Y's shape: torch.Size([759, 1])\n"
     ]
    }
   ],
   "source": [
    "# 구현한 커스텀 데이터셋 클래스의 인스턴스 생성\n",
    "dataset = DiabetesDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9b7c7de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 클래스를 사용하여 학습에 사용되는 객체 생성 \n",
    "dataloader = DataLoader(dataset, batch_size=100, shuffle=True,\n",
    "                       num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e93d55",
   "metadata": {},
   "source": [
    "## 퀴즈 (Easy)  \n",
    "당뇨병 데이터셋에 대해서 로지스틱 회귀 모델을 구현한다면 첫번째 레이어의 노드 수는 몇개가 되어야 할까요??  답변해보세요!!  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9bc9ec",
   "metadata": {},
   "source": [
    "## 퀴즈 (Normal)  \n",
    "이전의 노트북 파일을 참고해서 LogisticRegressionModel 클래스를 구현하세요.  \n",
    "1) 생성자 :  \n",
    "모델의 은닉층 수는 3층이고 은닉층마다 (8, 6, 4) 개의 뉴런을 가집니다.  \n",
    "마지막 레이어에는 시그모이드 모듈을 이용해 비선형변환을 수행하세요.  \n",
    "2) forward :   \n",
    "최종적으로 확률 값을 예측하도록 레이어를 쌓아서 y를 반환하세요.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e1fefff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.L1 = nn.Linear(8, 6)\n",
    "        self.L2 = nn.Linear(6, 4)\n",
    "        self.L3 = nn.Linear(4, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z1 = self.sigmoid(self.L1(x))\n",
    "        z2 = self.sigmoid(self.L2(z1))\n",
    "        z3 = self.sigmoid(self.L3(z2))\n",
    "        y_pred = self.sigmoid(z3)\n",
    "        return y_pred\n",
    "\n",
    "model = LogisticRegressionModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dd26f4",
   "metadata": {},
   "source": [
    "## 퀴즈 (Easy)  \n",
    "위 모델을 학습시키기 위해서는 어떤 손실함수를 선택해야할까요?? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c2e34bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss = nn.BCELoss(reduction='mean')\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090ae091",
   "metadata": {},
   "source": [
    "## 퀴즈 (Easy)  \n",
    "반복문에서 enumerate를 반드시 사용해야합니다.  \n",
    "그 이유가 무엇일까요?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "301c5b4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ye200\\anaconda3\\envs\\env\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1/10 Batch    1/8                 Loss: 0.642540\n",
      "Epoch:    1/10 Batch    2/8                 Loss: 0.614771\n",
      "Epoch:    1/10 Batch    3/8                 Loss: 0.625879\n",
      "Epoch:    1/10 Batch    4/8                 Loss: 0.670300\n",
      "Epoch:    1/10 Batch    5/8                 Loss: 0.637009\n",
      "Epoch:    1/10 Batch    6/8                 Loss: 0.698061\n",
      "Epoch:    1/10 Batch    7/8                 Loss: 0.648008\n",
      "Epoch:    1/10 Batch    8/8                 Loss: 0.623117\n",
      "Epoch:    2/10 Batch    1/8                 Loss: 0.664765\n",
      "Epoch:    2/10 Batch    2/8                 Loss: 0.614765\n",
      "Epoch:    2/10 Batch    3/8                 Loss: 0.648163\n",
      "Epoch:    2/10 Batch    4/8                 Loss: 0.636842\n",
      "Epoch:    2/10 Batch    5/8                 Loss: 0.637043\n",
      "Epoch:    2/10 Batch    6/8                 Loss: 0.686935\n",
      "Epoch:    2/10 Batch    7/8                 Loss: 0.647997\n",
      "Epoch:    2/10 Batch    8/8                 Loss: 0.623160\n",
      "Epoch:    3/10 Batch    1/8                 Loss: 0.675756\n",
      "Epoch:    3/10 Batch    2/8                 Loss: 0.698207\n",
      "Epoch:    3/10 Batch    3/8                 Loss: 0.659279\n",
      "Epoch:    3/10 Batch    4/8                 Loss: 0.631345\n",
      "Epoch:    3/10 Batch    5/8                 Loss: 0.636933\n",
      "Epoch:    3/10 Batch    6/8                 Loss: 0.664767\n",
      "Epoch:    3/10 Batch    7/8                 Loss: 0.581326\n",
      "Epoch:    3/10 Batch    8/8                 Loss: 0.604314\n",
      "Epoch:    4/10 Batch    1/8                 Loss: 0.675895\n",
      "Epoch:    4/10 Batch    2/8                 Loss: 0.653617\n",
      "Epoch:    4/10 Batch    3/8                 Loss: 0.592421\n",
      "Epoch:    4/10 Batch    4/8                 Loss: 0.675958\n",
      "Epoch:    4/10 Batch    5/8                 Loss: 0.620291\n",
      "Epoch:    4/10 Batch    6/8                 Loss: 0.659091\n",
      "Epoch:    4/10 Batch    7/8                 Loss: 0.625803\n",
      "Epoch:    4/10 Batch    8/8                 Loss: 0.679739\n",
      "Epoch:    5/10 Batch    1/8                 Loss: 0.625708\n",
      "Epoch:    5/10 Batch    2/8                 Loss: 0.692591\n",
      "Epoch:    5/10 Batch    3/8                 Loss: 0.636897\n",
      "Epoch:    5/10 Batch    4/8                 Loss: 0.625800\n",
      "Epoch:    5/10 Batch    5/8                 Loss: 0.603635\n",
      "Epoch:    5/10 Batch    6/8                 Loss: 0.675879\n",
      "Epoch:    5/10 Batch    7/8                 Loss: 0.659273\n",
      "Epoch:    5/10 Batch    8/8                 Loss: 0.651387\n",
      "Epoch:    6/10 Batch    1/8                 Loss: 0.642435\n",
      "Epoch:    6/10 Batch    2/8                 Loss: 0.625687\n",
      "Epoch:    6/10 Batch    3/8                 Loss: 0.609108\n",
      "Epoch:    6/10 Batch    4/8                 Loss: 0.664768\n",
      "Epoch:    6/10 Batch    5/8                 Loss: 0.636993\n",
      "Epoch:    6/10 Batch    6/8                 Loss: 0.681541\n",
      "Epoch:    6/10 Batch    7/8                 Loss: 0.642551\n",
      "Epoch:    6/10 Batch    8/8                 Loss: 0.679643\n",
      "Epoch:    7/10 Batch    1/8                 Loss: 0.625714\n",
      "Epoch:    7/10 Batch    2/8                 Loss: 0.631429\n",
      "Epoch:    7/10 Batch    3/8                 Loss: 0.681420\n",
      "Epoch:    7/10 Batch    4/8                 Loss: 0.603481\n",
      "Epoch:    7/10 Batch    5/8                 Loss: 0.653679\n",
      "Epoch:    7/10 Batch    6/8                 Loss: 0.670331\n",
      "Epoch:    7/10 Batch    7/8                 Loss: 0.664838\n",
      "Epoch:    7/10 Batch    8/8                 Loss: 0.632478\n",
      "Epoch:    8/10 Batch    1/8                 Loss: 0.609087\n",
      "Epoch:    8/10 Batch    2/8                 Loss: 0.653727\n",
      "Epoch:    8/10 Batch    3/8                 Loss: 0.625802\n",
      "Epoch:    8/10 Batch    4/8                 Loss: 0.636883\n",
      "Epoch:    8/10 Batch    5/8                 Loss: 0.687021\n",
      "Epoch:    8/10 Batch    6/8                 Loss: 0.631399\n",
      "Epoch:    8/10 Batch    7/8                 Loss: 0.642503\n",
      "Epoch:    8/10 Batch    8/8                 Loss: 0.707806\n",
      "Epoch:    9/10 Batch    1/8                 Loss: 0.642465\n",
      "Epoch:    9/10 Batch    2/8                 Loss: 0.642391\n",
      "Epoch:    9/10 Batch    3/8                 Loss: 0.659190\n",
      "Epoch:    9/10 Batch    4/8                 Loss: 0.636837\n",
      "Epoch:    9/10 Batch    5/8                 Loss: 0.642526\n",
      "Epoch:    9/10 Batch    6/8                 Loss: 0.592328\n",
      "Epoch:    9/10 Batch    7/8                 Loss: 0.709590\n",
      "Epoch:    9/10 Batch    8/8                 Loss: 0.641844\n",
      "Epoch:   10/10 Batch    1/8                 Loss: 0.631366\n",
      "Epoch:   10/10 Batch    2/8                 Loss: 0.642572\n",
      "Epoch:   10/10 Batch    3/8                 Loss: 0.664744\n",
      "Epoch:   10/10 Batch    4/8                 Loss: 0.636913\n",
      "Epoch:   10/10 Batch    5/8                 Loss: 0.687065\n",
      "Epoch:   10/10 Batch    6/8                 Loss: 0.608997\n",
      "Epoch:   10/10 Batch    7/8                 Loss: 0.659298\n",
      "Epoch:   10/10 Batch    8/8                 Loss: 0.632253\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        x_train, y_train = samples\n",
    "        y_pred = model(x_train)\n",
    "        loss = bce_loss(y_pred, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch: {epoch+1:4d}/{epochs} Batch {batch_idx+1:4d}/{len(dataloader)} \\\n",
    "                Loss: {loss.item():4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081f3ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5420198",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
