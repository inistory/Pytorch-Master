{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50d4fe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a935b7fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist_data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST\\raw\\train-images-idx3-ubyte.gz to ./mnist_data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST\\raw\\train-labels-idx1-ubyte.gz to ./mnist_data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "94.4%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80075430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
      "torch.Size([60000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(train_dataset.classes)\n",
    "print(train_dataset.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ba6b50",
   "metadata": {},
   "source": [
    "## 퀴즈 (Easy)  \n",
    "Multi Layered Perceptrion 을 통해 MNIST 데이터셋을 분류하려면  \n",
    "1) 모델의 첫 번째 레이어의 입력크기  \n",
    "2) 모델의 출력크기는 어떻게 되어야할까요?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46bdfa5",
   "metadata": {},
   "source": [
    "## 퀴즈 (Easy)  \n",
    "간단한 MLP 모델을 구현해봅시다. \n",
    "1) 은닉층은 총 4개로 (784, 512, 256, 128) 개의 뉴런이 존재합니다. \n",
    "  \n",
    "  \n",
    "2) 활성화함수는 relu를 사용합니다.  \n",
    "\n",
    "3) forward 함수에 x를 처음 입력받을 때 기존에 배운 flatten() 또는 reshape() 또는 view()를 활용해서 일차원 벡터로 변환하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2d7a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = nn.Linear(784, 512)\n",
    "        self.l2 = nn.Linear(512, 256)\n",
    "        self.l3 = nn.Linear(256, 128)\n",
    "        self.l4 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Flatten the data (n, 1, 28, 28)-> (n, 784)\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        y_pred = F.relu(self.l4(x))\n",
    "\n",
    "        return y_pred\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec2207",
   "metadata": {},
   "source": [
    "이제 train, test 함수를 작성해보겠습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1c193525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "model = Model().to(device)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "lr = 0.01\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "def train(epoch, model, loss_func, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_index, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_index % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} | Batch Status: {batch_index*len(x)}/{len(train_loader.dataset)} \\\n",
    "            ({100. * batch_index * batch_size / len(train_loader.dataset):.0f}% | Loss: {loss.item():.6f}')\n",
    "\n",
    "def test(model, loss_func, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_count = 0\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        #print(y)\n",
    "        y_pred = model(x)\n",
    "        #print(f\"y_pred shape: {y_pred.shape}\")\n",
    "        #print(f\"y_pred : {y_pred}\")\n",
    "        test_loss += loss_func(y_pred, y).item()\n",
    "        pred = y_pred.data.max(1, keepdim=True)[1]\n",
    "        # torch.eq : Computes element-wise equality. return counts value\n",
    "         \n",
    "        #print(f\"pred : {pred}\")\n",
    "        #print(f\"view_as : {y.data.view_as(pred)}\")\n",
    "        correct_count += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'=======================\\n Test set: Average loss: {test_loss:.4f}, Accuracy: {correct_count}/{len(test_loader.dataset)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f9834776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================\n",
      " Test set: Average loss: 0.0721, Accuracy: 1039/10000\n"
     ]
    }
   ],
   "source": [
    "test(model, ce_loss, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35e0616c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 | Batch Status: 0/60000             (0% | Loss: 2.298524\n",
      "Train Epoch: 1 | Batch Status: 3200/60000             (0% | Loss: 2.305743\n",
      "Train Epoch: 1 | Batch Status: 6400/60000             (0% | Loss: 2.300616\n",
      "Train Epoch: 1 | Batch Status: 9600/60000             (0% | Loss: 2.299551\n",
      "Train Epoch: 1 | Batch Status: 12800/60000             (1% | Loss: 2.295114\n",
      "Train Epoch: 1 | Batch Status: 16000/60000             (1% | Loss: 2.264555\n",
      "Train Epoch: 1 | Batch Status: 19200/60000             (1% | Loss: 2.265674\n",
      "Train Epoch: 1 | Batch Status: 22400/60000             (1% | Loss: 2.262136\n",
      "Train Epoch: 1 | Batch Status: 25600/60000             (1% | Loss: 2.181224\n",
      "Train Epoch: 1 | Batch Status: 28800/60000             (2% | Loss: 2.192803\n",
      "Train Epoch: 1 | Batch Status: 32000/60000             (2% | Loss: 2.137085\n",
      "Train Epoch: 1 | Batch Status: 35200/60000             (2% | Loss: 2.137395\n",
      "Train Epoch: 1 | Batch Status: 38400/60000             (2% | Loss: 1.955891\n",
      "Train Epoch: 1 | Batch Status: 41600/60000             (2% | Loss: 1.788056\n",
      "Train Epoch: 1 | Batch Status: 44800/60000             (2% | Loss: 1.671197\n",
      "Train Epoch: 1 | Batch Status: 48000/60000             (2% | Loss: 1.441239\n",
      "Train Epoch: 1 | Batch Status: 51200/60000             (3% | Loss: 1.424485\n",
      "Train Epoch: 1 | Batch Status: 54400/60000             (3% | Loss: 1.153460\n",
      "Train Epoch: 1 | Batch Status: 57600/60000             (3% | Loss: 1.354440\n",
      "=======================\n",
      " Test set: Average loss: 0.0397, Accuracy: 5982/10000\n",
      "Train Epoch: 2 | Batch Status: 0/60000             (0% | Loss: 0.979523\n",
      "Train Epoch: 2 | Batch Status: 3200/60000             (0% | Loss: 0.938420\n",
      "Train Epoch: 2 | Batch Status: 6400/60000             (0% | Loss: 1.409685\n",
      "Train Epoch: 2 | Batch Status: 9600/60000             (0% | Loss: 1.308688\n",
      "Train Epoch: 2 | Batch Status: 12800/60000             (1% | Loss: 1.019243\n",
      "Train Epoch: 2 | Batch Status: 16000/60000             (1% | Loss: 1.010781\n",
      "Train Epoch: 2 | Batch Status: 19200/60000             (1% | Loss: 0.909579\n",
      "Train Epoch: 2 | Batch Status: 22400/60000             (1% | Loss: 1.343889\n",
      "Train Epoch: 2 | Batch Status: 25600/60000             (1% | Loss: 1.080710\n",
      "Train Epoch: 2 | Batch Status: 28800/60000             (2% | Loss: 0.977597\n",
      "Train Epoch: 2 | Batch Status: 32000/60000             (2% | Loss: 0.986120\n",
      "Train Epoch: 2 | Batch Status: 35200/60000             (2% | Loss: 0.632876\n",
      "Train Epoch: 2 | Batch Status: 38400/60000             (2% | Loss: 0.822038\n",
      "Train Epoch: 2 | Batch Status: 41600/60000             (2% | Loss: 0.865713\n",
      "Train Epoch: 2 | Batch Status: 44800/60000             (2% | Loss: 0.805394\n",
      "Train Epoch: 2 | Batch Status: 48000/60000             (2% | Loss: 0.791420\n",
      "Train Epoch: 2 | Batch Status: 51200/60000             (3% | Loss: 1.042275\n",
      "Train Epoch: 2 | Batch Status: 54400/60000             (3% | Loss: 1.015312\n",
      "Train Epoch: 2 | Batch Status: 57600/60000             (3% | Loss: 0.807220\n",
      "=======================\n",
      " Test set: Average loss: 0.0305, Accuracy: 6423/10000\n",
      "Train Epoch: 3 | Batch Status: 0/60000             (0% | Loss: 1.211664\n",
      "Train Epoch: 3 | Batch Status: 3200/60000             (0% | Loss: 1.307121\n",
      "Train Epoch: 3 | Batch Status: 6400/60000             (0% | Loss: 0.937989\n",
      "Train Epoch: 3 | Batch Status: 9600/60000             (0% | Loss: 0.817632\n",
      "Train Epoch: 3 | Batch Status: 12800/60000             (1% | Loss: 0.646088\n",
      "Train Epoch: 3 | Batch Status: 16000/60000             (1% | Loss: 1.020610\n",
      "Train Epoch: 3 | Batch Status: 19200/60000             (1% | Loss: 1.175781\n",
      "Train Epoch: 3 | Batch Status: 22400/60000             (1% | Loss: 0.755875\n",
      "Train Epoch: 3 | Batch Status: 25600/60000             (1% | Loss: 0.717927\n",
      "Train Epoch: 3 | Batch Status: 28800/60000             (2% | Loss: 0.907692\n",
      "Train Epoch: 3 | Batch Status: 32000/60000             (2% | Loss: 0.765549\n",
      "Train Epoch: 3 | Batch Status: 35200/60000             (2% | Loss: 0.823519\n",
      "Train Epoch: 3 | Batch Status: 38400/60000             (2% | Loss: 1.135917\n",
      "Train Epoch: 3 | Batch Status: 41600/60000             (2% | Loss: 0.882682\n",
      "Train Epoch: 3 | Batch Status: 44800/60000             (2% | Loss: 1.348624\n",
      "Train Epoch: 3 | Batch Status: 48000/60000             (2% | Loss: 0.695467\n",
      "Train Epoch: 3 | Batch Status: 51200/60000             (3% | Loss: 1.325606\n",
      "Train Epoch: 3 | Batch Status: 54400/60000             (3% | Loss: 0.934924\n",
      "Train Epoch: 3 | Batch Status: 57600/60000             (3% | Loss: 0.874238\n",
      "=======================\n",
      " Test set: Average loss: 0.0288, Accuracy: 6496/10000\n",
      "Train Epoch: 4 | Batch Status: 0/60000             (0% | Loss: 0.691422\n",
      "Train Epoch: 4 | Batch Status: 3200/60000             (0% | Loss: 0.724132\n",
      "Train Epoch: 4 | Batch Status: 6400/60000             (0% | Loss: 0.916194\n",
      "Train Epoch: 4 | Batch Status: 9600/60000             (0% | Loss: 0.640567\n",
      "Train Epoch: 4 | Batch Status: 12800/60000             (1% | Loss: 0.709019\n",
      "Train Epoch: 4 | Batch Status: 16000/60000             (1% | Loss: 0.540310\n",
      "Train Epoch: 4 | Batch Status: 19200/60000             (1% | Loss: 0.870448\n",
      "Train Epoch: 4 | Batch Status: 22400/60000             (1% | Loss: 0.620829\n",
      "Train Epoch: 4 | Batch Status: 25600/60000             (1% | Loss: 0.832739\n",
      "Train Epoch: 4 | Batch Status: 28800/60000             (2% | Loss: 0.885167\n",
      "Train Epoch: 4 | Batch Status: 32000/60000             (2% | Loss: 0.977700\n",
      "Train Epoch: 4 | Batch Status: 35200/60000             (2% | Loss: 0.737760\n",
      "Train Epoch: 4 | Batch Status: 38400/60000             (2% | Loss: 0.872222\n",
      "Train Epoch: 4 | Batch Status: 41600/60000             (2% | Loss: 0.607082\n",
      "Train Epoch: 4 | Batch Status: 44800/60000             (2% | Loss: 0.923782\n",
      "Train Epoch: 4 | Batch Status: 48000/60000             (2% | Loss: 1.023706\n",
      "Train Epoch: 4 | Batch Status: 51200/60000             (3% | Loss: 0.985127\n",
      "Train Epoch: 4 | Batch Status: 54400/60000             (3% | Loss: 1.524758\n",
      "Train Epoch: 4 | Batch Status: 57600/60000             (3% | Loss: 1.007947\n",
      "=======================\n",
      " Test set: Average loss: 0.0275, Accuracy: 6603/10000\n",
      "Train Epoch: 5 | Batch Status: 0/60000             (0% | Loss: 0.828134\n",
      "Train Epoch: 5 | Batch Status: 3200/60000             (0% | Loss: 1.158707\n",
      "Train Epoch: 5 | Batch Status: 6400/60000             (0% | Loss: 1.042451\n",
      "Train Epoch: 5 | Batch Status: 9600/60000             (0% | Loss: 0.812899\n",
      "Train Epoch: 5 | Batch Status: 12800/60000             (1% | Loss: 0.767567\n",
      "Train Epoch: 5 | Batch Status: 16000/60000             (1% | Loss: 0.976444\n",
      "Train Epoch: 5 | Batch Status: 19200/60000             (1% | Loss: 0.941115\n",
      "Train Epoch: 5 | Batch Status: 22400/60000             (1% | Loss: 0.680276\n",
      "Train Epoch: 5 | Batch Status: 25600/60000             (1% | Loss: 0.748446\n",
      "Train Epoch: 5 | Batch Status: 28800/60000             (2% | Loss: 0.645034\n",
      "Train Epoch: 5 | Batch Status: 32000/60000             (2% | Loss: 0.755131\n",
      "Train Epoch: 5 | Batch Status: 35200/60000             (2% | Loss: 1.138140\n",
      "Train Epoch: 5 | Batch Status: 38400/60000             (2% | Loss: 0.828471\n",
      "Train Epoch: 5 | Batch Status: 41600/60000             (2% | Loss: 0.909867\n",
      "Train Epoch: 5 | Batch Status: 44800/60000             (2% | Loss: 0.693646\n",
      "Train Epoch: 5 | Batch Status: 48000/60000             (2% | Loss: 1.117101\n",
      "Train Epoch: 5 | Batch Status: 51200/60000             (3% | Loss: 0.746771\n",
      "Train Epoch: 5 | Batch Status: 54400/60000             (3% | Loss: 1.212632\n",
      "Train Epoch: 5 | Batch Status: 57600/60000             (3% | Loss: 0.675629\n",
      "=======================\n",
      " Test set: Average loss: 0.0267, Accuracy: 6655/10000\n",
      "Train Epoch: 6 | Batch Status: 0/60000             (0% | Loss: 0.852516\n",
      "Train Epoch: 6 | Batch Status: 3200/60000             (0% | Loss: 0.868284\n",
      "Train Epoch: 6 | Batch Status: 6400/60000             (0% | Loss: 0.334762\n",
      "Train Epoch: 6 | Batch Status: 9600/60000             (0% | Loss: 0.810549\n",
      "Train Epoch: 6 | Batch Status: 12800/60000             (1% | Loss: 0.628817\n",
      "Train Epoch: 6 | Batch Status: 16000/60000             (1% | Loss: 0.754188\n",
      "Train Epoch: 6 | Batch Status: 19200/60000             (1% | Loss: 0.708722\n",
      "Train Epoch: 6 | Batch Status: 22400/60000             (1% | Loss: 0.869483\n",
      "Train Epoch: 6 | Batch Status: 25600/60000             (1% | Loss: 0.810913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 | Batch Status: 28800/60000             (2% | Loss: 0.535518\n",
      "Train Epoch: 6 | Batch Status: 32000/60000             (2% | Loss: 0.947575\n",
      "Train Epoch: 6 | Batch Status: 35200/60000             (2% | Loss: 0.835803\n",
      "Train Epoch: 6 | Batch Status: 38400/60000             (2% | Loss: 1.090175\n",
      "Train Epoch: 6 | Batch Status: 41600/60000             (2% | Loss: 1.055956\n",
      "Train Epoch: 6 | Batch Status: 44800/60000             (2% | Loss: 0.842598\n",
      "Train Epoch: 6 | Batch Status: 48000/60000             (2% | Loss: 0.827892\n",
      "Train Epoch: 6 | Batch Status: 51200/60000             (3% | Loss: 1.066991\n",
      "Train Epoch: 6 | Batch Status: 54400/60000             (3% | Loss: 1.072013\n",
      "Train Epoch: 6 | Batch Status: 57600/60000             (3% | Loss: 0.777363\n",
      "=======================\n",
      " Test set: Average loss: 0.0261, Accuracy: 6701/10000\n",
      "Train Epoch: 7 | Batch Status: 0/60000             (0% | Loss: 0.958400\n",
      "Train Epoch: 7 | Batch Status: 3200/60000             (0% | Loss: 1.128547\n",
      "Train Epoch: 7 | Batch Status: 6400/60000             (0% | Loss: 0.770817\n",
      "Train Epoch: 7 | Batch Status: 9600/60000             (0% | Loss: 0.659255\n",
      "Train Epoch: 7 | Batch Status: 12800/60000             (1% | Loss: 1.047469\n",
      "Train Epoch: 7 | Batch Status: 16000/60000             (1% | Loss: 0.888260\n",
      "Train Epoch: 7 | Batch Status: 19200/60000             (1% | Loss: 1.163673\n",
      "Train Epoch: 7 | Batch Status: 22400/60000             (1% | Loss: 0.707077\n",
      "Train Epoch: 7 | Batch Status: 25600/60000             (1% | Loss: 0.849895\n",
      "Train Epoch: 7 | Batch Status: 28800/60000             (2% | Loss: 0.677541\n",
      "Train Epoch: 7 | Batch Status: 32000/60000             (2% | Loss: 0.853532\n",
      "Train Epoch: 7 | Batch Status: 35200/60000             (2% | Loss: 0.645594\n",
      "Train Epoch: 7 | Batch Status: 38400/60000             (2% | Loss: 0.849115\n",
      "Train Epoch: 7 | Batch Status: 41600/60000             (2% | Loss: 0.565758\n",
      "Train Epoch: 7 | Batch Status: 44800/60000             (2% | Loss: 0.839301\n",
      "Train Epoch: 7 | Batch Status: 48000/60000             (2% | Loss: 1.182008\n",
      "Train Epoch: 7 | Batch Status: 51200/60000             (3% | Loss: 0.875928\n",
      "Train Epoch: 7 | Batch Status: 54400/60000             (3% | Loss: 0.788882\n",
      "Train Epoch: 7 | Batch Status: 57600/60000             (3% | Loss: 0.714912\n",
      "=======================\n",
      " Test set: Average loss: 0.0257, Accuracy: 6724/10000\n",
      "Train Epoch: 8 | Batch Status: 0/60000             (0% | Loss: 0.809071\n",
      "Train Epoch: 8 | Batch Status: 3200/60000             (0% | Loss: 0.717318\n",
      "Train Epoch: 8 | Batch Status: 6400/60000             (0% | Loss: 1.070436\n",
      "Train Epoch: 8 | Batch Status: 9600/60000             (0% | Loss: 0.331188\n",
      "Train Epoch: 8 | Batch Status: 12800/60000             (1% | Loss: 0.953641\n",
      "Train Epoch: 8 | Batch Status: 16000/60000             (1% | Loss: 0.594053\n",
      "Train Epoch: 8 | Batch Status: 19200/60000             (1% | Loss: 0.802916\n",
      "Train Epoch: 8 | Batch Status: 22400/60000             (1% | Loss: 0.764924\n",
      "Train Epoch: 8 | Batch Status: 25600/60000             (1% | Loss: 0.680630\n",
      "Train Epoch: 8 | Batch Status: 28800/60000             (2% | Loss: 1.660402\n",
      "Train Epoch: 8 | Batch Status: 32000/60000             (2% | Loss: 0.680511\n",
      "Train Epoch: 8 | Batch Status: 35200/60000             (2% | Loss: 0.647527\n",
      "Train Epoch: 8 | Batch Status: 38400/60000             (2% | Loss: 0.793184\n",
      "Train Epoch: 8 | Batch Status: 41600/60000             (2% | Loss: 0.857602\n",
      "Train Epoch: 8 | Batch Status: 44800/60000             (2% | Loss: 0.682049\n",
      "Train Epoch: 8 | Batch Status: 48000/60000             (2% | Loss: 0.785624\n",
      "Train Epoch: 8 | Batch Status: 51200/60000             (3% | Loss: 0.567969\n",
      "Train Epoch: 8 | Batch Status: 54400/60000             (3% | Loss: 0.736612\n",
      "Train Epoch: 8 | Batch Status: 57600/60000             (3% | Loss: 0.576305\n",
      "=======================\n",
      " Test set: Average loss: 0.0253, Accuracy: 6749/10000\n",
      "Train Epoch: 9 | Batch Status: 0/60000             (0% | Loss: 0.899407\n",
      "Train Epoch: 9 | Batch Status: 3200/60000             (0% | Loss: 0.470264\n",
      "Train Epoch: 9 | Batch Status: 6400/60000             (0% | Loss: 0.651951\n",
      "Train Epoch: 9 | Batch Status: 9600/60000             (0% | Loss: 0.706304\n",
      "Train Epoch: 9 | Batch Status: 12800/60000             (1% | Loss: 0.898263\n",
      "Train Epoch: 9 | Batch Status: 16000/60000             (1% | Loss: 0.603090\n",
      "Train Epoch: 9 | Batch Status: 19200/60000             (1% | Loss: 0.690583\n",
      "Train Epoch: 9 | Batch Status: 22400/60000             (1% | Loss: 0.640070\n",
      "Train Epoch: 9 | Batch Status: 25600/60000             (1% | Loss: 1.127871\n",
      "Train Epoch: 9 | Batch Status: 28800/60000             (2% | Loss: 0.568746\n",
      "Train Epoch: 9 | Batch Status: 32000/60000             (2% | Loss: 1.042780\n",
      "Train Epoch: 9 | Batch Status: 35200/60000             (2% | Loss: 1.157291\n",
      "Train Epoch: 9 | Batch Status: 38400/60000             (2% | Loss: 0.956688\n",
      "Train Epoch: 9 | Batch Status: 41600/60000             (2% | Loss: 0.695979\n",
      "Train Epoch: 9 | Batch Status: 44800/60000             (2% | Loss: 0.954637\n",
      "Train Epoch: 9 | Batch Status: 48000/60000             (2% | Loss: 1.125198\n",
      "Train Epoch: 9 | Batch Status: 51200/60000             (3% | Loss: 0.742027\n",
      "Train Epoch: 9 | Batch Status: 54400/60000             (3% | Loss: 0.791304\n",
      "Train Epoch: 9 | Batch Status: 57600/60000             (3% | Loss: 1.076289\n",
      "=======================\n",
      " Test set: Average loss: 0.0249, Accuracy: 6768/10000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,10):\n",
    "    train(epoch, model, ce_loss, train_loader, optimizer)\n",
    "    test(model, ce_loss, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67959d1",
   "metadata": {},
   "source": [
    "# MNIST Classification with CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304ec08e",
   "metadata": {},
   "source": [
    "앞서, MLP 모델을 통해 MNIST 데이터셋을 분류해보았지만 성능이 좋지 않았습니다.  \n",
    "이를 개선하기 위해 CNN을 직접 구현해봅시다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bcc89935",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, C, W, H, K, S):\n",
    "        super(CNN, self).__init__()\n",
    "        # nn.Module에는 이미 Conv 레이어가 구현되어 있습니다.\n",
    "        # 마찬가지로 배치정규화 레이어도 구현되어 있습니다.\n",
    "        self.conv1 = nn.Conv2d(C, 16, kernel_size=K, stride=S)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=K, stride=S)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=K, stride=S)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size=K, stride=S):\n",
    "            print((size - (kernel_size - 1) - 1) // stride + 1)\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "        convw = conv2d_size_out(W, K, S)\n",
    "        convw = conv2d_size_out(convw, K, S)        \n",
    "        convw = conv2d_size_out(convw, K, S)        \n",
    "        \n",
    "        self.linear_input_size = convw * convw * 64\n",
    "        self.fc = nn.Linear(self.linear_input_size, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1) # (batch_size, flaaten_size)\n",
    "        x = F.relu(self.fc(x))\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "679d7cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "6\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "model = CNN(C=1, W=28, H=28, K=3, S=2) \n",
    "model = model.to(device)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "770b4ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(epoch, model, train_loader, optimizer, loss_func):\n",
    "    model.train()\n",
    "    for batch_index, samples in enumerate(train_loader):\n",
    "        data, target = samples\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(data)\n",
    "        loss = loss_func(pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_index % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} | Batch Status: {batch_index*len(data)}/{len(train_loader.dataset)} \\\n",
    "            ({100.0 * batch_index * batch_size / len(train_loader.dataset):.0f}% | Loss: {loss.item():.6f}')\n",
    "\n",
    "            \n",
    "def test_cnn(model, test_loader, loss_func):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_counts = 0\n",
    "    for _, samples in enumerate(test_loader):\n",
    "        data, target = samples\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        loss = loss_func(output, target)\n",
    "        test_loss += loss.item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct_counts += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "  \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'=======================\\n Test set: Average loss: {test_loss:.4f}, Accuracy: {correct_counts}/{len(test_loader.dataset)}' \\\n",
    "          f'({100.*correct_counts/len(test_loader.dataset):.0f}%)')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0e3934f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ye200\\anaconda3\\envs\\env\\lib\\site-packages\\ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 | Batch Status: 0/60000             (0% | Loss: 0.360748\n",
      "Train Epoch: 1 | Batch Status: 3200/60000             (5% | Loss: 0.435026\n",
      "Train Epoch: 1 | Batch Status: 6400/60000             (11% | Loss: 0.470100\n",
      "Train Epoch: 1 | Batch Status: 9600/60000             (16% | Loss: 0.381106\n",
      "Train Epoch: 1 | Batch Status: 12800/60000             (21% | Loss: 0.525765\n",
      "Train Epoch: 1 | Batch Status: 16000/60000             (27% | Loss: 0.433752\n",
      "Train Epoch: 1 | Batch Status: 19200/60000             (32% | Loss: 0.363350\n",
      "Train Epoch: 1 | Batch Status: 22400/60000             (37% | Loss: 0.803605\n",
      "Train Epoch: 1 | Batch Status: 25600/60000             (43% | Loss: 0.537166\n",
      "Train Epoch: 1 | Batch Status: 28800/60000             (48% | Loss: 0.478825\n",
      "Train Epoch: 1 | Batch Status: 32000/60000             (53% | Loss: 0.438385\n",
      "Train Epoch: 1 | Batch Status: 35200/60000             (59% | Loss: 0.656166\n",
      "Train Epoch: 1 | Batch Status: 38400/60000             (64% | Loss: 0.216127\n",
      "Train Epoch: 1 | Batch Status: 41600/60000             (69% | Loss: 0.424324\n",
      "Train Epoch: 1 | Batch Status: 44800/60000             (75% | Loss: 0.154847\n",
      "Train Epoch: 1 | Batch Status: 48000/60000             (80% | Loss: 0.516203\n",
      "Train Epoch: 1 | Batch Status: 51200/60000             (85% | Loss: 0.145957\n",
      "Train Epoch: 1 | Batch Status: 54400/60000             (91% | Loss: 0.575939\n",
      "Train Epoch: 1 | Batch Status: 57600/60000             (96% | Loss: 0.676276\n",
      "=======================\n",
      " Test set: Average loss: 0.0155, Accuracy: 7944/10000(79%)\n",
      "Train Epoch: 2 | Batch Status: 0/60000             (0% | Loss: 0.303157\n",
      "Train Epoch: 2 | Batch Status: 3200/60000             (5% | Loss: 0.707208\n",
      "Train Epoch: 2 | Batch Status: 6400/60000             (11% | Loss: 0.580289\n",
      "Train Epoch: 2 | Batch Status: 9600/60000             (16% | Loss: 0.288541\n",
      "Train Epoch: 2 | Batch Status: 12800/60000             (21% | Loss: 0.360350\n",
      "Train Epoch: 2 | Batch Status: 16000/60000             (27% | Loss: 0.305970\n",
      "Train Epoch: 2 | Batch Status: 19200/60000             (32% | Loss: 0.432301\n",
      "Train Epoch: 2 | Batch Status: 22400/60000             (37% | Loss: 0.392411\n",
      "Train Epoch: 2 | Batch Status: 25600/60000             (43% | Loss: 0.146254\n",
      "Train Epoch: 2 | Batch Status: 28800/60000             (48% | Loss: 0.675626\n",
      "Train Epoch: 2 | Batch Status: 32000/60000             (53% | Loss: 0.681681\n",
      "Train Epoch: 2 | Batch Status: 35200/60000             (59% | Loss: 0.440176\n",
      "Train Epoch: 2 | Batch Status: 38400/60000             (64% | Loss: 0.654475\n",
      "Train Epoch: 2 | Batch Status: 41600/60000             (69% | Loss: 0.216929\n",
      "Train Epoch: 2 | Batch Status: 44800/60000             (75% | Loss: 0.647698\n",
      "Train Epoch: 2 | Batch Status: 48000/60000             (80% | Loss: 0.615855\n",
      "Train Epoch: 2 | Batch Status: 51200/60000             (85% | Loss: 0.345037\n",
      "Train Epoch: 2 | Batch Status: 54400/60000             (91% | Loss: 0.432193\n",
      "Train Epoch: 2 | Batch Status: 57600/60000             (96% | Loss: 0.625322\n",
      "=======================\n",
      " Test set: Average loss: 0.0152, Accuracy: 7957/10000(80%)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,3):\n",
    "    train_cnn(epoch, model, train_loader, optimizer, ce_loss)\n",
    "    test_cnn(model, test_loader, ce_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c095428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_layers, block, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=16, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # feature map size = 32x32x16\n",
    "        self.layers_2n = self.get_layers(block, 16, 16, stride=1)\n",
    "        # feature map size = 16x16x32\n",
    "        self.layers_4n = self.get_layers(block, 16, 32, stride=2)\n",
    "        # feature map size = 8x8x64\n",
    "        self.layers_6n = self.get_layers(block, 32, 64, stride=2)\n",
    "\n",
    "        # output layers\n",
    "        # self.avg_pool = nn.AvgPool2d(8, stride=1)\n",
    "        self.fc_out = nn.Linear(49 * 64, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out',\n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def get_layers(self, block, in_channels, out_channels, stride):\n",
    "        if stride == 2:\n",
    "            down_sample = True\n",
    "        else:\n",
    "            down_sample = False\n",
    "\n",
    "        layers_list = nn.ModuleList(\n",
    "            [block(in_channels, out_channels, stride, down_sample)])\n",
    "\n",
    "        for _ in range(self.num_layers - 1):\n",
    "            layers_list.append(block(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layers_2n(x)\n",
    "        x = self.layers_4n(x)\n",
    "        x = self.layers_6n(x)\n",
    "\n",
    "        #x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, down_sample=False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.stride = stride\n",
    "\n",
    "        if down_sample:\n",
    "            self.down_sample = IdentityPadding(in_channels, out_channels, stride)\n",
    "        else:\n",
    "            self.down_sample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.down_sample is not None:\n",
    "            shortcut = self.down_sample(x)\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class IdentityPadding(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(IdentityPadding, self).__init__()\n",
    "\n",
    "        self.pooling = nn.MaxPool2d(1, stride=stride)\n",
    "        self.add_channels = out_channels - in_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))\n",
    "        out = self.pooling(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet_model():\n",
    "    block = ResidualBlock\n",
    "    model = ResNet(1, 5, block)\n",
    "    return model\n",
    "\n",
    "model = resnet_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c78c02ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 | Batch Status: 0/60000             (0% | Loss: 3.523117\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22792/3944671196.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain_cnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mce_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtest_cnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mce_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22792/3199768193.py\u001b[0m in \u001b[0;36mtrain_cnn\u001b[1;34m(epoch, model, train_loader, optimizer, loss_func)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ye200\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ye200\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1,3):\n",
    "    train_cnn(epoch, model, train_loader, optimizer, ce_loss)\n",
    "    test_cnn(model, test_loader, ce_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166ce772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
